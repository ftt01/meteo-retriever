{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "lib_dir = \"/home/daniele/documents/github/ftt01/phd/share/lib\"\n",
    "sys.path.insert( 0, lib_dir )\n",
    "\n",
    "from lib import *\n",
    "import subprocess\n",
    "import psycopg2\n",
    "# import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class local_args():\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def add_start_date(self,  start_date):\n",
    "        self.start_date = start_date\n",
    "    \n",
    "    def add_end_date(self,  end_date):\n",
    "        self.end_date = end_date\n",
    "\n",
    "    def add_variable(self,  variable):\n",
    "        self.variable = variable\n",
    "    \n",
    "    def add_meta_grid(self,  meta_grid):\n",
    "        self.meta_grid = meta_grid\n",
    "\n",
    "    def add_output_path(self,  output_path):\n",
    "        self.output_path = output_path\n",
    "        mkNestedDir( output_path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "input_parser = argparse.ArgumentParser()\n",
    "input_parser.add_argument('start_date', type=str)\n",
    "input_parser.add_argument('end_date', type=str)\n",
    "input_parser.add_argument('variable', type=str)\n",
    "input_parser.add_argument('output_path', type=str)\n",
    "input_parser.add_argument('meta_grid', type=str)\n",
    "args = input_parser.parse_args()\n",
    "# except:\n",
    "#     args = local_args()\n",
    "#     args.add_start_date(\"2010-01-01T00:00:00\")\n",
    "#     args.add_end_date(\"2019-12-31T23:59:00\")\n",
    "#     args.add_variable(\"2t\")\n",
    "#     args.add_output_path(\"/media/windows/projects/bias_correction/applications/era5/data/pre_processed/\")\n",
    "#     args.add_meta_grid(\"/media/windows/projects/bias_correction/applications/era5/data/pre_processed/grid.csv\")\n",
    "    # args.add_meta_grid(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = dt.datetime.strptime( args.start_date, '%Y-%m-%dT%H:%M:%S' )\n",
    "end_date = dt.datetime.strptime( args.end_date, '%Y-%m-%dT%H:%M:%S' )\n",
    "aggregation_at = '1H'\n",
    "dates = pd.date_range(start_date, end_date, freq=aggregation_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_postgres_connection():\n",
    "\n",
    "    db_name = 'meteo'\n",
    "    db_user = 'postgres'\n",
    "    db_password = 'pgAifa2Bias?'\n",
    "    db_host = '172.20.0.2'\n",
    "\n",
    "    return psycopg2.connect(database=db_name, user=db_user, password=db_password, host=db_host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_point( y, x, epsg=4326, tolerance=0.01 ):\n",
    "\n",
    "    c_id = None\n",
    "\n",
    "    sql_exist = '''\n",
    "        SELECT COUNT(*)\n",
    "        FROM ecmwf.era5_points\n",
    "        WHERE ST_Contains(\n",
    "            ST_Transform(\n",
    "                ST_MakeEnvelope({min_lon}, {min_lat}, {max_lon}, {max_lat}, {epsg}), 4326 ),\n",
    "            era5_points.geom)\n",
    "        LIMIT 1;'''\n",
    "\n",
    "    min_lat = y - tolerance\n",
    "    max_lat = y + tolerance\n",
    "    min_lon = x - tolerance\n",
    "    max_lon = x + tolerance\n",
    "\n",
    "    sql_exist = sql_exist.format(\n",
    "        min_lat=min_lat,\n",
    "        min_lon=min_lon,\n",
    "        max_lat=max_lat,\n",
    "        max_lon=max_lon,\n",
    "        epsg=epsg\n",
    "    )\n",
    "\n",
    "    sql_select = '''\n",
    "        SELECT ecmwf.era5_points.id\n",
    "        FROM ecmwf.era5_points\n",
    "        ORDER BY ecmwf.era5_points.geom <#> ST_SetSRID(ST_MakePoint({x},{y}),{epsg})\n",
    "        LIMIT 1;'''.format(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            epsg=epsg\n",
    "        )\n",
    "    \n",
    "    # print(sql_select)\n",
    "\n",
    "    conn = get_postgres_connection()\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(sql_exist)\n",
    "            rows = cur.fetchall()\n",
    "            if rows[0][0] != 0:\n",
    "                cur.execute(sql_select)\n",
    "                c_id = int(cur.fetchall()[0][0])\n",
    "            else:\n",
    "                print(\"Not existing point!\")\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "    return c_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_to_dataframe(conn, query, column_names):\n",
    "\n",
    "    # print(query)\n",
    "    cursor = conn.cursor()\n",
    "    try:\n",
    "        cursor.execute(query)\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(\"Error: %s\" % error)\n",
    "        cursor.close()\n",
    "        return 1\n",
    "    # The execute returns a list of tuples:\n",
    "    tuples_list = cursor.fetchall()\n",
    "    cursor.close()\n",
    "    # Now we need to transform the list into a pandas DataFrame:\n",
    "    df = pd.DataFrame(tuples_list, columns=column_names)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.DataFrame(index=dates)\n",
    "full_df.index.name = 'datetime'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_get_data_metadata_tp = '''\n",
    "SELECT datetime, value\n",
    "FROM ecmwf.era5_values\n",
    "WHERE datetime >= '{start_datetime}' \n",
    "    AND datetime <= '{end_datetime}' \n",
    "    AND variable = '{variable}'\n",
    "    AND point = {c_id}\n",
    "GROUP BY datetime, value\n",
    "ORDER BY datetime\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_get_data_geom_tp = '''\n",
    "WITH inner_points AS (\n",
    "    WITH poly AS (\n",
    "        SELECT ST_Buffer(geom::geography, 5000)::geometry as geom\n",
    "        FROM geometries.eu_ita_regions\n",
    "        WHERE name = 'Trentino-Alto Adige'\n",
    "    ),\n",
    "    points AS (\n",
    "        SELECT id, geom\n",
    "        FROM ecmwf.era5_points\n",
    "    )\n",
    "    SELECT points.id as id, points.geom\n",
    "    FROM points, poly\n",
    "    WHERE ST_Contains(poly.geom, points.geom)\n",
    ")\n",
    "SELECT point, datetime, value\n",
    "FROM ecmwf.era5_values, inner_points\n",
    "WHERE datetime >= '{start_datetime}' \n",
    "    AND datetime <= '{end_datetime}' \n",
    "    AND variable = '{variable}'\n",
    "    AND point = inner_points.id\n",
    "GROUP BY point, datetime, value\n",
    "ORDER BY point, datetime\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_get_data_metadata_2t = '''\n",
    "SELECT datetime, value \n",
    "FROM ecmwf.era5_values\n",
    "WHERE datetime >= '{start_datetime}' \n",
    "    AND datetime <= '{end_datetime}' \n",
    "    AND variable = '{variable}'\n",
    "    AND point = {c_id}\n",
    "ORDER BY datetime\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_get_data_geom_2t = '''\n",
    "WITH inner_points AS (\n",
    "    WITH poly AS (\n",
    "        SELECT ST_Buffer(geom::geography, 5000)::geometry as geom\n",
    "        FROM geometries.eu_ita_regions\n",
    "        WHERE name = 'Trentino-Alto Adige'\n",
    "    ),\n",
    "    points AS (\n",
    "        SELECT id, geom\n",
    "        FROM ecmwf.era5_points\n",
    "    )\n",
    "    SELECT points.id as id, points.geom\n",
    "    FROM points, poly\n",
    "    WHERE ST_Contains(poly.geom, points.geom)\n",
    ")\n",
    "SELECT point, datetime, value\n",
    "FROM ecmwf.era5_values, inner_points\n",
    "WHERE datetime >= '{start_datetime}' \n",
    "    AND datetime <= '{end_datetime}' \n",
    "    AND variable = '{variable}'\n",
    "    AND point = inner_points.id\n",
    "GROUP BY point, datetime, value\n",
    "ORDER BY point, datetime\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(args.meta_grid, 'r') as ff:\n",
    "        lines = ff.readlines()[1:]\n",
    "        ff.close()\n",
    "\n",
    "    for l in lines:\n",
    "\n",
    "        splitted = l.split(',')\n",
    "        old_id = int(splitted[0])\n",
    "        lat = float(splitted[1])\n",
    "        lon = float(splitted[2])\n",
    "\n",
    "        c_id = get_point(lat,lon)\n",
    "\n",
    "        if args.variable == 'tp':\n",
    "            sql_get_data = sql_get_data_metadata_tp.format(\n",
    "                    start_datetime = (start_date-dt.timedelta(hours=1)).strftime('%Y-%m-%d %H:%M'),\n",
    "                    end_datetime = end_date.strftime('%Y-%m-%d %H:%M'),\n",
    "                    variable = args.variable,\n",
    "                    c_id = c_id\n",
    "                )\n",
    "        elif args.variable == '2t':\n",
    "            sql_get_data = sql_get_data_metadata_2t.format(\n",
    "                    start_datetime = start_date.strftime('%Y-%m-%d %H:%M'),\n",
    "                    end_datetime = end_date.strftime('%Y-%m-%d %H:%M'),\n",
    "                    variable = args.variable,\n",
    "                    c_id = c_id\n",
    "                )\n",
    "        c_df = sql_to_dataframe( get_postgres_connection(), sql_get_data, column_names = ['datetime', 'value'])\n",
    "        c_df.set_index('datetime', inplace=True)\n",
    "        ### put the old_id or the c_id in the columns of full_df\n",
    "        c_df.rename(columns={'value':old_id}, inplace=True)\n",
    "        \n",
    "        ## precipitation\n",
    "        if args.variable == 'tp':\n",
    "            c_df = resample_timeseries( \n",
    "                c_df, res_type='sum', \n",
    "                step=aggregation_at, offset=False )\n",
    "            # from meters to mm\n",
    "            c_df = c_df * 1000\n",
    "            c_df = c_df[start_date:end_date]\n",
    "        ## temperature\n",
    "        elif args.variable == '2t':\n",
    "            c_df = resample_timeseries( \n",
    "                c_df, res_type='mean', \n",
    "                step=aggregation_at, offset=False )\n",
    "            # from Kelvin to Celsius\n",
    "            c_df = c_df - 273.15\n",
    "\n",
    "        full_df = pd.concat([full_df,c_df], axis=1, join='inner')\n",
    "except:\n",
    "    print(\"No metadata file, using DB IDs\")\n",
    "\n",
    "    if args.variable == 'tp':\n",
    "        sql_get_data = sql_get_data_geom_tp.format(\n",
    "                start_datetime = (start_date-dt.timedelta(hours=1)).strftime('%Y-%m-%d %H:%M'),\n",
    "                end_datetime = end_date.strftime('%Y-%m-%d %H:%M'),\n",
    "                variable = args.variable\n",
    "            )\n",
    "    elif args.variable == '2t':\n",
    "        sql_get_data = sql_get_data_geom_2t.format(\n",
    "                start_datetime = start_date.strftime('%Y-%m-%d %H:%M'),\n",
    "                end_datetime = end_date.strftime('%Y-%m-%d %H:%M'),\n",
    "                variable = args.variable\n",
    "            )\n",
    "    c_full_df = sql_to_dataframe( get_postgres_connection(), sql_get_data, column_names = ['point', 'datetime', 'value'])\n",
    "\n",
    "    points = pd.unique(c_full_df['point'])\n",
    "    print('N. of points: ' + str(len(points)))\n",
    "    for c_id in points:\n",
    "        c_df = c_full_df[ c_full_df['point']==c_id ]\n",
    "\n",
    "        c_df = c_df[['datetime','value']]\n",
    "        c_df.set_index('datetime', inplace=True)\n",
    "        ### put the old_id or the c_id in the columns of full_df\n",
    "        c_df.rename(columns={'value':c_id}, inplace=True)\n",
    "        \n",
    "        ## precipitation\n",
    "        if args.variable == 'tp':\n",
    "            c_df = resample_timeseries( \n",
    "                c_df, res_type='sum', \n",
    "                step=aggregation_at, offset=True )\n",
    "            # from meters to mm\n",
    "            c_df = c_df * 1000\n",
    "            c_df = c_df[start_date:end_date]\n",
    "        ## temperature\n",
    "        elif args.variable == '2t':\n",
    "            c_df = resample_timeseries( \n",
    "                c_df, res_type='mean', \n",
    "                step=aggregation_at, offset=True )\n",
    "            # from Kelvin to Celsius\n",
    "            c_df = c_df - 273.15\n",
    "\n",
    "        full_df = pd.concat([full_df,c_df], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = full_df.round(decimals=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.output_path + args.variable + '_' + start_date.strftime('%Y%m%dT%H%M%S') + '_' + end_date.strftime('%Y%m%dT%H%M%S') + '.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.to_csv( args.output_path + args.variable + '_' + start_date.strftime('%Y%m%dT%H%M%S') + '_' + end_date.strftime('%Y%m%dT%H%M%S') + '.csv' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### CHECK INSTANT\n",
    "# sql_get_data = '''\n",
    "#             SELECT datetime, value*1000 as mm, point\n",
    "# \tFROM ecmwf.era5_values\n",
    "# \tWHERE datetime >= '2010-01-01 00:00' AND datetime <= '2010-12-31 23:59' AND point = 18 AND variable ='tp'\n",
    "#     '''\n",
    "\n",
    "# c_df = sql_to_dataframe( get_postgres_connection(), sql_get_data, column_names = ['datetime', 'mm', 'point'])\n",
    "# c_df['datetime'] = pd.to_datetime(c_df['datetime'])\n",
    "# c_df.set_index('datetime',inplace=True)\n",
    "# c_df['mm'] = [float(n) for n in c_df['mm'].values]\n",
    "\n",
    "# inst = [ float(0) ]\n",
    "# for el in c_df.index:\n",
    "#     if el.hour == 1:\n",
    "#         inst.append( c_df.loc[el]['mm'] )\n",
    "#     else:\n",
    "#         try:\n",
    "#             inst.append( abs(c_df.loc[el]['mm'] - c_df.loc[el-dt.timedelta(hours=1)]['mm']) )\n",
    "#         except:\n",
    "#             print('Skipped first')\n",
    "\n",
    "# c_df['inst'] = inst\n",
    "# c_df.replace('Y').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### EXTRACTION TEMPERATURE\n",
    "# sql_2t_get_data = '''\n",
    "\n",
    "# WITH instant_data as (\n",
    "# \tWITH c_data as (\n",
    "# \t\tSELECT datetime, value, point\n",
    "# \t\tFROM ecmwf.era5_values\n",
    "# \t\tWHERE variable ='2t'\n",
    "# \t\tORDER BY point,datetime\n",
    "# \t),\n",
    "# \tinner_points AS (\n",
    "# \t\tWITH poly AS (\n",
    "# \t\t\tSELECT geom\n",
    "# \t\t\tFROM geometries.pranav\n",
    "# \t\t\tWHERE name = 'TAA_15km'\n",
    "# \t\t),\n",
    "# \t\tpoints AS (\n",
    "# \t\t\tSELECT id, geom\n",
    "# \t\t\tFROM ecmwf.era5_points\n",
    "# \t\t)\n",
    "# \t\tSELECT points.id as id, points.geom\n",
    "# \t\tFROM points, poly\n",
    "# \t\tWHERE ST_Contains(poly.geom, points.geom)\n",
    "# \t)\n",
    "# \tSELECT c_data.value-273.15 AS celsius, \n",
    "# \t\tinner_points.id as id, inner_points.geom as geom, c_data.datetime as dt\n",
    "# \tFROM c_data, inner_points\n",
    "# \tWHERE inner_points.id = c_data.point\n",
    "# \tGROUP BY c_data.value,inner_points.id,c_data.datetime,inner_points.geom\n",
    "# \tORDER BY inner_points.id,c_data.datetime\n",
    "# )\n",
    "# SELECT EXTRACT(year FROM instant_data.dt) AS year, ROUND(AVG(instant_data.celsius),2) AS yearly_mean,\n",
    "# \tinstant_data.id as point, ST_X(instant_data.geom) AS lon, ST_Y(instant_data.geom) AS lat\n",
    "# FROM instant_data \n",
    "# GROUP BY year,point,geom\n",
    "# ORDER BY point;\n",
    "\n",
    "# '''\n",
    "# temp_df = sql_to_dataframe( get_postgres_connection(), sql_2t_get_data, column_names = ['year', 'yearly_mean', 'point', 'lon', 'lat'])\n",
    "# temp_df.to_csv(\"/media/windows/projects/kriging/era5land/temperature_yearly_means.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### EXTRACTION PRECIPITATION\n",
    "# sql_tp_get_data = '''\n",
    "\n",
    "# WITH instant_data as (\n",
    "# \tWITH c_data as (\n",
    "# \t\tSELECT datetime, value, point\n",
    "# \t\tFROM ecmwf.era5_values\n",
    "# \t\tWHERE variable ='tp'\n",
    "# \t-- \t\tAND datetime >= '2010-01-01 00:00' AND datetime <= '2010-12-31 23:59'\n",
    "# -- \t\tGROUP BY datetime, value\n",
    "# \t\tORDER BY point,datetime\n",
    "# \t),\n",
    "# \tinner_points AS (\n",
    "# \t\tWITH poly AS (\n",
    "# -- \t\t\tSELECT ST_Buffer(geom,1000) as geom\n",
    "# -- \t\t\tFROM geometries.eu_ita_regions\n",
    "# -- \t\t\tWHERE name = 'Trentino-Alto Adige'\n",
    "# \t\t\tSELECT geom\n",
    "# \t\t\tFROM geometries.pranav\n",
    "# \t\t\tWHERE name = 'TAA_15km'\n",
    "# \t\t),\n",
    "# \t\tpoints AS (\n",
    "# \t\t\tSELECT id, geom\n",
    "# \t\t\tFROM ecmwf.era5_points\n",
    "# \t\t)\n",
    "# \t\tSELECT points.id as id, points.geom\n",
    "# \t\tFROM points, poly\n",
    "# \t\tWHERE ST_Contains(poly.geom, points.geom)\n",
    "# \t)\n",
    "# \tSELECT \n",
    "# \t\tCASE\n",
    "# \t\t\tWHEN \n",
    "# -- \t\t\t\tEXTRACT(hour FROM c_data.datetime) <\n",
    "# -- \t\t\t\tEXTRACT(hour FROM LAG(c_data.datetime) OVER (ORDER BY inner_points.id,c_data.datetime))\n",
    "# \t\t\t\tEXTRACT(hour FROM c_data.datetime) = 1\n",
    "# \t\t\tTHEN c_data.value*1000\n",
    "# \t\t\tELSE (c_data.value*1000 - LAG(c_data.value*1000) OVER (ORDER BY inner_points.id,c_data.datetime))\n",
    "# \t\tEND AS instant, \n",
    "# \t\tinner_points.id as id, inner_points.geom as geom, c_data.datetime as dt\n",
    "# \tFROM c_data, inner_points\n",
    "# \tWHERE inner_points.id = c_data.point\n",
    "# \tGROUP BY c_data.value,inner_points.id,c_data.datetime,inner_points.geom\n",
    "# \tORDER BY inner_points.id,c_data.datetime\n",
    "# )\n",
    "# SELECT EXTRACT(year FROM instant_data.dt) AS year, ROUND(SUM(instant_data.instant),2) AS yearly_mean,\n",
    "# \tinstant_data.id as point, ST_X(instant_data.geom) AS lon, ST_Y(instant_data.geom) AS lat\n",
    "# FROM instant_data \n",
    "# WHERE instant_data.instant >= 0\n",
    "# GROUP BY year,point,geom\n",
    "# ORDER BY point;\n",
    "\n",
    "# '''\n",
    "\n",
    "# prec_df = sql_to_dataframe( get_postgres_connection(), sql_tp_get_data, column_names = ['year', 'yearly_mean', 'point', 'lon', 'lat'])\n",
    "# prec_df.to_csv(\"/media/windows/projects/kriging/era5land/precipitation_yearly_sums.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql_a_muzzo = '''WITH inner_points AS (\n",
    "#     WITH poly AS (\n",
    "#         SELECT ST_Buffer(geom::geography, 5000)::geometry as geom\n",
    "#         FROM geometries.eu_ita_regions\n",
    "#         WHERE name = 'Trentino-Alto Adige'\n",
    "#     ),\n",
    "#     points AS (\n",
    "#         SELECT id, geom\n",
    "#         FROM ecmwf.era5_points\n",
    "#     )\n",
    "#     SELECT points.id as id, points.geom\n",
    "#     FROM points, poly\n",
    "#     WHERE ST_Contains(poly.geom, points.geom)\n",
    "# )\n",
    "# SELECT inner_points.id, ST_X(inner_points.geom) as lon, ST_Y(inner_points.geom) as lat\n",
    "# FROM inner_points'''\n",
    "\n",
    "# c_df = sql_to_dataframe( get_postgres_connection(), sql_a_muzzo, column_names = ['id', 'lon', 'lat'])\n",
    "# c_df.set_index('id', inplace=True)\n",
    "# c_df.to_csv(\"/media/windows/projects/bias_correction/applications/era5/data/pre_processed/grid.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
