{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "lib_dir = \"/home/daniele/documents/github/ftt01/phd/share/lib\"\n",
    "sys.path.insert( 0, lib_dir )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_data( df_datetime_values, datetime_format_output, output_path_filename ):\n",
    "\n",
    "    df_datetime_values.reset_index(inplace=True)\n",
    "    df = df_datetime_values.copy()\n",
    "    \n",
    "    df.loc[:,['datetime']] = df['datetime'].apply(lambda x: dt.datetime.strftime( x, datetime_format_output ) )\n",
    "    df.loc[:,['values']] = df['values'].apply(lambda y: round( y,2 ))\n",
    "\n",
    "    df.set_index('datetime',inplace=True)\n",
    "\n",
    "    mkNestedDir( os.path.dirname(output_path_filename) )\n",
    "    df.to_csv( output_path_filename )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    input_parser = argparse.ArgumentParser()\n",
    "    input_parser.add_argument('configuration_file', type=str)\n",
    "    args = input_parser.parse_args()\n",
    "    configuration_file = args.configuration_file\n",
    "except:\n",
    "    configuration_file = \"../../etc/conf/extract/config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(configuration_file) as config_file:\n",
    "    configuration = json.load(config_file)\n",
    "\n",
    "    project_name = configuration[\"project_name\"]\n",
    "\n",
    "    provider_name = configuration[\"provider_name\"]\n",
    "    model_name = configuration[\"model\"][\"name\"]\n",
    "    n_ensemble = configuration[\"model\"][\"ensemble\"]\n",
    "    lead_hours = configuration[\"model\"][\"lead_hours\"]\n",
    "    releases = configuration[\"model\"][\"releases\"]\n",
    "    \n",
    "    input_path = configuration[\"input_path\"]\n",
    "    output_path = configuration[\"output_path\"]\n",
    "    mkNestedDir(output_path)\n",
    "    log_path = Path( configuration[\"log_path\"] )\n",
    "    mkNestedDir(log_path)\n",
    "\n",
    "    start_date = configuration[\"start_date\"]\n",
    "    end_date = configuration[\"end_date\"]\n",
    "    datetime_format_input = configuration[\"datetime_format_input\"]\n",
    "\n",
    "    datetime_format_output = configuration[\"datetime_format_output\"]\n",
    "    current_tz = configuration[\"timezone\"]\n",
    "    \n",
    "    if configuration[\"logging_level\"] == \"info\":\n",
    "        logging_level = logging.INFO\n",
    "    elif configuration[\"logging_level\"] == \"debug\":\n",
    "        logging_level = logging.DEBUG\n",
    "    else:\n",
    "        logging_level = logging.ERROR\n",
    "    \n",
    "    script_version = configuration[\"script_version\"]\n",
    "\n",
    "    roi_config_file = configuration[\"roi_config\"]\n",
    "    with open(roi_config_file) as roi_config_f:\n",
    "        roi_configuration = json.load(roi_config_f)\n",
    "\n",
    "        roi_key = roi_configuration[\"main\"][\"key\"]\n",
    "        roi_name = roi_configuration[\"main\"][\"name\"]\n",
    "        basins = roi_configuration[\"basins\"]\n",
    "\n",
    "    roi_config_f.close()\n",
    "\n",
    "config_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    filename = str(log_path) + \"/\" + provider_name + \"_extract.log\",\n",
    "    format = '%(asctime)s - %(message)s',\n",
    "    filemode = 'a',\n",
    "    level = logging_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "computation_start = dt.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info( \"Project name: \" + project_name )\n",
    "logging.info( \"Provider name: \" + provider_name )\n",
    "\n",
    "logging.info( \"Input path: \" + input_path )\n",
    "logging.info( \"Output path: \" + output_path )\n",
    "logging.info( \"Log filename path: \" + str(log_path) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lead_hours == -1:\n",
    "    start_datetime = dt.datetime.strptime( start_date + 'T00:00:00', '%Y%m%dT%H:%M:%S' ).replace(tzinfo=tz.gettz(current_tz))\n",
    "    end_datetime = dt.datetime.strptime( end_date + 'T23:59:59', '%Y%m%dT%H:%M:%S', ).replace(tzinfo=tz.gettz(current_tz))\n",
    "    days_to_compute = [ dt.datetime.strftime( start_datetime + dt.timedelta(days=i), format='%Y%m%d' ) for i in range( (end_datetime - start_datetime).days+1 ) ]\n",
    "\n",
    "    logging.info( \"Extract from: \" + str(start_datetime) )\n",
    "    logging.info( \"Extract to: \" + str(end_datetime) )\n",
    "else:\n",
    "    start_date = dt.datetime.strptime( start_date, '%Y%m%d' ).replace(tzinfo=tz.gettz(current_tz))\n",
    "    end_date = dt.datetime.strptime( end_date, '%Y%m%d' ).replace(tzinfo=tz.gettz(current_tz))\n",
    "\n",
    "    logging.info( \"Extract from: \" + str(start_date) )\n",
    "    logging.info( \"Extract to: \" + str(end_date) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_basins = glob.glob( input_path + \"*/\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subbasin in basins:\n",
    "\n",
    "    subbasin_key = subbasin['key']\n",
    "    subbasin_name = subbasin['name']\n",
    "    logging.info( \"Processing subbasin: \" + subbasin_key + \" | \" + subbasin_name )\n",
    "\n",
    "    for stations in subbasin['ground_stations']:\n",
    "        \n",
    "        variable = stations['variable']\n",
    "        logging.info( \"Variable: \" + variable )\n",
    "        station_id = stations['station_id']\n",
    "        logging.info( \"Station ID: \" + str(station_id) )\n",
    "\n",
    "        try:\n",
    "            c_data = pd.read_csv( input_path + variable + \"/\" + str(station_id) + \".csv\" )\n",
    "        except:\n",
    "            logging.error( \"Station ID not valid: \" + str(station_id) )\n",
    "            continue\n",
    "        \n",
    "        c_data['datetime'] = [ t.replace(tzinfo=tz.gettz(current_tz)) for t in pd.to_datetime( c_data['datetime'], format=datetime_format_input ) ]\n",
    "        c_data.set_index( 'datetime', inplace=True )\n",
    "\n",
    "        if lead_hours == -1:\n",
    "\n",
    "            to_export_data = pd.DataFrame( index=pd.date_range( \n",
    "                start=start_datetime, end=end_datetime, freq='H' )\n",
    "            )\n",
    "            to_export_data.index.name = 'datetime'\n",
    "            to_export_data = pd.merge( \n",
    "                to_export_data,c_data[start_datetime:end_datetime], \n",
    "                how=\"left\", left_index=True, right_index=True )\n",
    "            \n",
    "            output_path_filename = output_path + roi_key + \"/\" + \\\n",
    "                subbasin_key + \"/\" + variable + \"/\" + \\\n",
    "                    dt.datetime.strftime( start_datetime, \"%Y%m%d\" ) + dt.datetime.strftime( end_datetime, \"%Y%m%d\" ) + \".csv\"\n",
    "            \n",
    "            export_data( to_export_data, datetime_format_output, output_path_filename )\n",
    "\n",
    "        else:\n",
    "            for rel in releases:\n",
    "                curr_start_datetime = start_date + dt.timedelta( hours=rel+1 )\n",
    "                while curr_start_datetime.date() <= end_date.date():\n",
    "\n",
    "                    curr_end_datetime = curr_start_datetime + dt.timedelta( hours=lead_hours-1 )\n",
    "                    \n",
    "                    logging.debug( \"Current start datetime: \" + str(curr_start_datetime) )\n",
    "                    logging.debug( \"Current end datetime: \" + str(curr_end_datetime) )\n",
    "                    \n",
    "                    to_export_data = pd.DataFrame( index=pd.date_range(\n",
    "                        start=curr_start_datetime, end=curr_end_datetime, freq='H' )\n",
    "                    )\n",
    "                    to_export_data.index.name = 'datetime'\n",
    "                    to_export_data = pd.merge( \n",
    "                        to_export_data,c_data[curr_start_datetime:curr_end_datetime], \n",
    "                        how=\"left\", left_index=True, right_index=True )\n",
    "\n",
    "                    output_path_filename = output_path + roi_key + \"/\" + \\\n",
    "                        subbasin_key + \"/\" + \"R{release}\".format(release=str(rel).zfill(3)) + \\\n",
    "                            \"/\" + variable + \"/\" + \\\n",
    "                                dt.datetime.strftime( curr_start_datetime, \"%Y%m%d\" ) + \".csv\"\n",
    "                    export_data( to_export_data, datetime_format_output, output_path_filename )\n",
    "                    del to_export_data\n",
    "\n",
    "                    curr_start_datetime = curr_start_datetime + dt.timedelta( days=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "send_email(\n",
    "    subject=\"Meteo Alto Adige extracted: \" + project_name,\n",
    "    body=\"Started at \" + computation_start.strftime(format=\"%Y-%m-%dT%H:%M:%SZ%z\") + \n",
    "        \"\\nFinish at \" + dt.datetime.now().strftime(format=\"%Y-%m-%dT%H:%M:%SZ%z\") +\n",
    "        \"\\nJSON config: \" + json.dumps(configuration, indent=2, default=str)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
