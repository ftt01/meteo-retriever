{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "lib_dir = \"/home/daniele/documents/github/ftt01/phd/share/lib\"\n",
    "sys.path.insert( 0, lib_dir )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_parser = argparse.ArgumentParser()\n",
    "input_parser.add_argument('configuration_file', type=str)\n",
    "args = input_parser.parse_args()\n",
    "configuration_file = args.configuration_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration_file = \"/home/daniele/documents/github/ftt01/phd/data/meteo/providers/dwd/etc/conf/extract/icon-d2-eps.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unuseful_grib2( path_dir, keywords=[], mirror=False ):\n",
    "    logging.info(\"Removing unuseful files in: \" + path_dir)\n",
    "\n",
    "    if mirror == True:\n",
    "        files_to_remove = glob.glob( path_dir + \"*.grib2\")\n",
    "        for k in keywords:\n",
    "            files_to_remove = list( set(files_to_remove) - set(glob.glob( path_dir + \"{kw}*.grib2\".format(kw=k))) )\n",
    "    else:\n",
    "        files_to_remove = []\n",
    "        for k in keywords:\n",
    "            files_to_remove = files_to_remove + list(set(glob.glob( path_dir + \"*{kw}*.grib2\".format(kw=k))))\n",
    "\n",
    "    if files_to_remove != None:\n",
    "        for f in files_to_remove:\n",
    "            logging.debug(\"Removing: \" + f)\n",
    "            os.remove( f )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## takes all files in the directory, that contains one of the keywords and apply the grib_copy\n",
    "## the results are saved in a directory called to_regrid\n",
    "## remove the unuseful files in to_regrid directory\n",
    "\n",
    "def reorder_grib2(path_dir, reorder_keywords=None, regrid_keywords=None):\n",
    "\n",
    "    mkNestedDir( path_dir + \"to_regrid/\" )\n",
    "\n",
    "    logging.info(\"Reordering using keywords: \" + str(reorder_keywords))\n",
    "\n",
    "    files_to_reorder = glob.glob(path_dir + \"*.grib2\")\n",
    "    for file_to_reorder in files_to_reorder:\n",
    "        if all(word in file_to_reorder for word in reorder_keywords):\n",
    "            subprocess.run('''grib_copy \\\n",
    "                {path_dir}{file}.grib2 \\\n",
    "                {path_dir}to_regrid/{file}_[stepRange].grib2'''.format(\n",
    "                    path_dir=path_dir,\n",
    "                    file=os.path.basename(file_to_reorder)[:-6]),\n",
    "                    shell=True, check=True, executable='/bin/bash')\n",
    "    \n",
    "    logging.info(\"Remove using keywords: \" + str(regrid_keywords))\n",
    "    \n",
    "    files_to_regrid = glob.glob(path_dir + \"to_regrid/\" + \"*.grib2\")\n",
    "    for file_to_regrid in files_to_regrid:\n",
    "        if any(word in file_to_regrid for word in regrid_keywords):\n",
    "            continue\n",
    "        else:\n",
    "            os.remove( file_to_regrid )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_to_regrid(path_dir, regrid_keywords=None, vars=None, mode='any'):\n",
    "\n",
    "    mkNestedDir( path_dir + \"to_regrid/\" )\n",
    "\n",
    "    logging.info(\"Moving to regrid using keywords: \" + str(regrid_keywords))\n",
    "\n",
    "    files_to_regrid = glob.glob(path_dir + \"*.grib2\")\n",
    "    for file_to_regrid in files_to_regrid:\n",
    "\n",
    "        for c_var in vars:\n",
    "\n",
    "            if not(c_var in file_to_regrid):\n",
    "                continue\n",
    "\n",
    "            if mode == 'any':\n",
    "\n",
    "                if any(word in file_to_regrid for word in regrid_keywords):\n",
    "                    try:\n",
    "                        shutil.copy(file_to_regrid, path_dir + \"to_regrid/\" + os.path.basename(file_to_regrid))\n",
    "                    except IOError as e:\n",
    "                        print(\"Unable to copy file. %s\" % e)\n",
    "                    except:\n",
    "                        print(\"Unexpected error:\", sys.exc_info())\n",
    "            \n",
    "            elif mode == 'all':\n",
    "\n",
    "                if all(word in file_to_regrid for word in regrid_keywords):\n",
    "                    try:\n",
    "                        shutil.copy(file_to_regrid, path_dir + \"to_regrid/\" + os.path.basename(file_to_regrid))\n",
    "                    except IOError as e:\n",
    "                        print(\"Unable to copy file. %s\" % e)\n",
    "                    except:\n",
    "                        print(\"Unexpected error:\", sys.exc_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regrid_dir(project_name, path_dir, model_name=None, keywords=None, remove_unregridded=False):\n",
    "\n",
    "    if keywords != None:\n",
    "\n",
    "        logging.info(\"Regridding using keywords: \" + str(keywords))\n",
    "\n",
    "        files_to_regrid = glob.glob(path_dir + \"*.grib2\")\n",
    "\n",
    "        for file_to_regrid in files_to_regrid:\n",
    "            if any(word in file_to_regrid for word in keywords):\n",
    "                subprocess.run('''docker run --name regrid_{model_name}_{prj} --rm                     --volume {path_dir}:/local                     --env INPUT_FILE=/local/{file}                     --env OUTPUT_FILE=/local/regridded_{file}                     deutscherwetterdienst/regrid:{model_name}                     /convert.sh'''.format(\n",
    "                    prj=project_name,\n",
    "                    model_name=model_name,\n",
    "                    path_dir=path_dir,\n",
    "                    file=os.path.basename(file_to_regrid)),\n",
    "                    shell=True, check=True,\n",
    "                    executable='/bin/bash')\n",
    "    else:\n",
    "\n",
    "        logging.info(\"Regridding entire directory: \" + path_dir)\n",
    "\n",
    "        subprocess.run('''docker run --name regrid_{model_name}_{prj} --rm                 --volume {path_dir}:/local                 --env INPUT_FILE=/local                 --env OUTPUT_FILE=/local                 deutscherwetterdienst/regrid:{model_name}                 /convert.sh'''.format(\n",
    "            prj=project_name,\n",
    "            path_dir=path_dir,\n",
    "            model_name=model_name),\n",
    "            shell=True, check=True,\n",
    "            executable='/bin/bash')\n",
    "\n",
    "    if remove_unregridded == True:\n",
    "        remove_unuseful_grib2(path_dir, keywords=['regridded'], mirror=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tranform_to_instant( current_df ):\n",
    "    ### the df in input is a timeseries of N ensemble on the columns\n",
    "    ### datetime,001,002,003,...,020\n",
    "\n",
    "    for c in current_df.columns:\n",
    "        values = current_df[c]\n",
    "        \n",
    "        val = []\n",
    "        val.append( values[0] )\n",
    "        for i in range(1, len(values)):\n",
    "            val.append( round(values[i] - values[i-1],2) )\n",
    "        \n",
    "        current_df[c] = val\n",
    "        del [val,values]\n",
    "    \n",
    "    return current_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_on_subbasins( \n",
    "    dir, df, variable, main_basin_name, rel_time, input_type,\n",
    "    output_datetime_format='%Y-%m-%dT%H:%M:%SZ%z', input_timezone=pytz.timezone('UTC'), \n",
    "    subbasins=[], model_resolution=0.02, generate_meta=True):\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    logging.info(\"Extraction of: \" + variable)\n",
    "    try:\n",
    "        lats = pd.unique( df['lat'] )\n",
    "        lons = pd.unique( df['lon'] )\n",
    "\n",
    "        for el in subbasins:\n",
    "            logging.info(\"Extraction on: \" + el['name'])\n",
    "\n",
    "            # curr_lat_min,curr_lat_max,curr_lon_min,curr_lon_max = bbox_extraction( \n",
    "            #     bbox_lat_min=el['bbox']['lat_min'],\n",
    "            #     bbox_lat_max=el['bbox']['lat_max'],\n",
    "            #     bbox_lon_min=el['bbox']['lon_min'],\n",
    "            #     bbox_lon_max=el['bbox']['lon_max'],\n",
    "            #     spatial_resolution=model_resolution,\n",
    "            #     lon_factor=0)\n",
    "            curr_lat_min = el['bbox']['lat_min']\n",
    "            curr_lat_max = el['bbox']['lat_max']\n",
    "            curr_lon_min = el['bbox']['lon_min']\n",
    "            curr_lon_max = el['bbox']['lon_max']\n",
    "            \n",
    "            logging.info(\"BBOX with buffer: [{lat_min},{lon_min},{lat_max},{lon_max}]\".format(\n",
    "                lat_min=curr_lat_min,\n",
    "                lon_min=curr_lon_min,\n",
    "                lat_max=curr_lat_max,\n",
    "                lon_max=curr_lon_max\n",
    "            ))\n",
    "\n",
    "            basin_lats = lats[ \n",
    "                (lats >= curr_lat_min) &\n",
    "                (lats <= curr_lat_max)]\n",
    "            basin_lons = lons[\n",
    "                (lons >= curr_lon_min) &\n",
    "                (lons <= curr_lon_max)]\n",
    "\n",
    "            id = 1\n",
    "\n",
    "            grid_ids = []\n",
    "            grid_lons = []\n",
    "            grid_lats = []\n",
    "\n",
    "            for lat in basin_lats:\n",
    "                for lon in basin_lons:\n",
    "                    \n",
    "                    current_df = df[(df['lat'] > lat-model_resolution/2) &\n",
    "                                    (df['lat'] < lat+model_resolution/2) &\n",
    "                                    (df['lon'] > lon-model_resolution/2) &\n",
    "                                    (df['lon'] < lon+model_resolution/2)]\n",
    "                    \n",
    "                    if current_df.empty:\n",
    "                        continue\n",
    "\n",
    "                    if generate_meta == True:\n",
    "                        grid_ids.append(id)\n",
    "                        grid_lons.append(lon)\n",
    "                        grid_lats.append(lat)\n",
    "\n",
    "                    current_df.loc[:,'ens'] = [str(i).zfill(3) for i in current_df['band']]\n",
    "                    current_df = current_df.drop(columns=['lat','lon','band'])\n",
    "                    current_df.set_index( 'ens',inplace=True )\n",
    "                    current_df = current_df.T\n",
    "                    # current_df.set_index( [df.iloc[0], df.columns[0]],inplace=True )\n",
    "                    \n",
    "                    c_idx = pd.to_datetime(current_df.index, utc=True)\n",
    "                    current_df.index = [ el.astimezone(input_timezone) for el in c_idx ]\n",
    "                    current_df.index.name = \"datetime\"\n",
    "\n",
    "                    if variable == 'precipitation':\n",
    "                        current_df = tranform_to_instant( current_df )\n",
    "\n",
    "                    output_file_path = dir + \"{main_basin}/{subbasin}/{release_hour}/{variable}/{input_type}/{point_id}/\".format(\n",
    "                        main_basin = main_basin_name,\n",
    "                        subbasin = el['key'],\n",
    "                        release_hour = \"R\" + str( rel_time ).zfill(3),\n",
    "                        variable = variable,\n",
    "                        input_type = input_type,\n",
    "                        point_id = str(id).zfill(3),\n",
    "                        )\n",
    "                    mkNestedDir(output_file_path)\n",
    "\n",
    "                    output_file_name = output_file_path + dt.datetime.strftime( current_df.index[0], format='%Y%m%d' ) + \".csv\" \n",
    "                    current_df.index = [ dt.datetime.strftime( ix, output_datetime_format ) for ix in current_df.index ]\n",
    "                    current_df.index.name = \"datetime\"\n",
    "                    current_df.to_csv( output_file_name )\n",
    "\n",
    "                    id = id + 1\n",
    "\n",
    "            if generate_meta == True:\n",
    "                meta_df = pd.DataFrame(index=grid_ids)\n",
    "                meta_df[\"lon\"] = grid_lons\n",
    "                meta_df[\"lat\"] = grid_lats\n",
    "\n",
    "                metadata_filename = Path(output_file_path)\n",
    "                meta_df.to_csv( str(metadata_filename.parent.absolute()) + \"/grid.csv\" )\n",
    "\n",
    "            del current_df\n",
    "    except:\n",
    "        logging.info(\"Uncomplete: \" + variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraction_call(file, type='rasterio'):\n",
    "\n",
    "    if type == 'rasterio':\n",
    "\n",
    "        ds = xr.open_dataset(file, engine=\"rasterio\")\n",
    "        # <xarray.Dataset>\n",
    "        # Dimensions:           (ensemble0: 20, lat_0: 745, lon_0: 1214)\n",
    "        # Coordinates:\n",
    "        # * lat_0             (lat_0) float32 43.18 43.2 43.22 ... 58.02 58.04 58.06\n",
    "        # * lon_0             (lon_0) float32 356.1 356.1 356.1 ... 380.3 380.3 380.3\n",
    "        # * ensemble0         (ensemble0) int32 0 1 2 3 4 5 6 7 ... 13 14 15 16 17 18 19\n",
    "        # Data variables:\n",
    "        #     TMP_P1_L103_GLL0  (ensemble0, lat_0, lon_0) float32 ...\n",
    "        #     ensemble0_info    (ensemble0) |S0 ...\n",
    "        variable_mapped_meta = ds.variables.get(list(ds.keys())[0]).attrs\n",
    "        # {\n",
    "        # 'center': 'Offenbach (RSMC)',\n",
    "        # 'production_status': 'Operational products',\n",
    "        # 'long_name': 'Temperature',\n",
    "        # 'units': 'K',\n",
    "        # 'grid_type': 'Latitude/longitude',\n",
    "        # 'parameter_discipline_and_category': 'Meteorological products, Temperature',\n",
    "        # 'parameter_template_discipline_category_number': array([1, 0, 0, 0], dtype = int32),\n",
    "        # 'level_type': 'Specified height level above ground (m)',\n",
    "        # 'level': array([2.], dtype = float32),\n",
    "        # 'forecast_time': array([60], dtype = int32),\n",
    "        # 'forecast_time_units': 'minutes',\n",
    "        # 'initial_time': '02/17/2021 (00:00)'\n",
    "        # }\n",
    "        current_datetime = dt.datetime.utcfromtimestamp(variable_mapped_meta['GRIB_VALID_TIME'])\n",
    "        ds_red = ds.sel(y=slice(roi_bbox_lat_max,roi_bbox_lat_min),x=slice(roi_bbox_lon_min,roi_bbox_lon_max))\n",
    "\n",
    "        lat_key = 'y'\n",
    "        lon_key = 'x'\n",
    "        ens_keyname = 'band'\n",
    "    \n",
    "    elif type == 'cfgrib':\n",
    "\n",
    "        ds = xr.open_dataset(file, engine=\"cfgrib\")\n",
    "        variable_mapped_meta = ds.variables.get(list(ds.keys())[0]).attrs\n",
    "        current_datetime = pd.Timestamp(ds.valid_time.values)\n",
    "        logging.debug(\"Extracting file related to UTC datetime: \" + str(current_datetime))\n",
    "        ds_red = ds.sel(latitude=slice(roi_bbox_lat_min,roi_bbox_lat_max),longitude=slice(roi_bbox_lon_min,roi_bbox_lon_max))\n",
    "\n",
    "        lat_key = 'latitude'\n",
    "        lon_key = 'longitude'\n",
    "        ens_keyname = 'number'\n",
    "    \n",
    "    variable_mapped_name = list(ds.keys())[0]\n",
    "    del [ds]\n",
    "\n",
    "    return current_datetime, ds_red, lat_key, lon_key, ens_keyname, variable_mapped_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_csv(files_to_convert, model_variables, logging, roi_bbox=[], implementation='local', input_path=None):\n",
    "\n",
    "    roi_bbox_lat_min = roi_bbox[0]\n",
    "    roi_bbox_lat_max = roi_bbox[1]\n",
    "    \n",
    "    roi_bbox_lon_min = roi_bbox[2]\n",
    "    roi_bbox_lon_max = roi_bbox[3]\n",
    "\n",
    "    if implementation=='local':\n",
    "\n",
    "        t_2m_df = pd.DataFrame()\n",
    "        tot_prec_df = pd.DataFrame()\n",
    "        w_snow_df = pd.DataFrame()\n",
    "\n",
    "        for file in files_to_convert:\n",
    "\n",
    "            logging.debug(\"Converting to csv: \" + file)\n",
    "            try:\n",
    "                logging.debug(\"Converting to csv using rasterio on: \" + file)\n",
    "                current_datetime, ds_red, lat_key, lon_key, ens_keyname, variable_mapped_name = extraction_call(file, type='rasterio')                \n",
    "            except:\n",
    "                logging.debug(\"Failed to convert to csv using rasterio on: \" + file)\n",
    "                logging.debug(\"Converting to csv using rasterio on: \" + file)\n",
    "                current_datetime, ds_red, lat_key, lon_key, ens_keyname, variable_mapped_name = extraction_call(file, type='cfgrib')\n",
    "            \n",
    "            df = ds_red.to_dataframe()\n",
    "            del [ds_red]\n",
    "\n",
    "            df.reset_index(inplace=True)\n",
    "            # df['lon'] = [round(idx,6) for idx in df['x']]\n",
    "            # df['lat'] = [round(idy,6) for idy in df['y']]\n",
    "\n",
    "            # release_time = ( current_datetime - dt.timedelta(hours=1) ).hour\n",
    "            # if not(release_time in model_releases):\n",
    "            #     continue\n",
    "\n",
    "            if model_variables[\"temperature\"] in file:\n",
    "                variable = 'temperature'\n",
    "                \n",
    "                if t_2m_df.empty:\n",
    "\n",
    "                    t_2m_df = df\n",
    "\n",
    "                    # t_2m_df.insert(0, 'id', range(1, 1 + len(df)))\n",
    "\n",
    "                    t_2m_df['lat'] = [round(idy,6) for idy in df[lat_key]]\n",
    "                    t_2m_df['lon'] = [round(idx,6) for idx in df[lon_key]]\n",
    "\n",
    "                    # t_2m_df.set_index(['lat','lon','band'], inplace=True)\n",
    "                    \n",
    "                    t_2m_df[current_datetime] = [round(idy,2) for idy in df[variable_mapped_name]]\n",
    "                    t_2m_df = t_2m_df.loc[:, ['lat','lon',ens_keyname,current_datetime]]\n",
    "                    t_2m_df = t_2m_df.rename(columns={ens_keyname:'band'})\n",
    "\n",
    "                else:\n",
    "                    t_2m_df[current_datetime] = [round(idy,2) for idy in df[variable_mapped_name]]\n",
    "            \n",
    "            elif model_variables[\"precipitation\"] in file:\n",
    "                variable = 'precipitation'\n",
    "                \n",
    "                if tot_prec_df.empty:\n",
    "\n",
    "                    tot_prec_df = df\n",
    "\n",
    "                    # tot_prec_df.insert(0, 'id', range(1, 1 + len(df)))\n",
    "\n",
    "                    tot_prec_df['lat'] = [round(idy,6) for idy in tot_prec_df[lat_key]]\n",
    "                    tot_prec_df['lon'] = [round(idx,6) for idx in tot_prec_df[lon_key]]\n",
    "\n",
    "                    # tot_prec_df.set_index(['lat','lon','band'], inplace=True)\n",
    "                    \n",
    "                    tot_prec_df[current_datetime] = [round(idy,2) for idy in df[variable_mapped_name]]\n",
    "                    tot_prec_df = tot_prec_df.loc[:, ['lat','lon',ens_keyname,current_datetime]]\n",
    "                    tot_prec_df = tot_prec_df.rename(columns={ens_keyname:'band'})\n",
    "\n",
    "                else:\n",
    "                    tot_prec_df[current_datetime] = [round(idy,2) for idy in df[variable_mapped_name]]\n",
    "\n",
    "            elif model_variables[\"snow\"] in file:\n",
    "                variable = 'snow'\n",
    "               \n",
    "                if w_snow_df.empty:\n",
    "\n",
    "                    w_snow_df = df\n",
    "\n",
    "                    # w_snow_df.insert(0, 'id', range(1, 1 + len(df)))\n",
    "\n",
    "                    w_snow_df['lat'] = [round(idy,6) for idy in w_snow_df[lat_key]]\n",
    "                    w_snow_df['lon'] = [round(idx,6) for idx in w_snow_df[lon_key]]\n",
    "\n",
    "                    # w_snow_df.set_index(['lat','lon','band'], inplace=True)\n",
    "                    \n",
    "                    w_snow_df[current_datetime] = [round(idy,2) for idy in df[variable_mapped_name]]\n",
    "                    w_snow_df = w_snow_df.loc[:, ['lat','lon',ens_keyname,current_datetime]]\n",
    "                    w_snow_df = w_snow_df.rename(columns={ens_keyname:'band'})\n",
    "\n",
    "                else:\n",
    "                    w_snow_df[current_datetime] = [round(idy,2) for idy in df[variable_mapped_name]]\n",
    "            else:\n",
    "                raise KeyError\n",
    "    \n",
    "    ### if the local implementation does not work, use pynio in docker\n",
    "    elif implementation == 'docker':\n",
    "\n",
    "        ## run a docker on a list of files, the input_path and the output_pathnames\n",
    "        # docker run --rm -v /media/windows/data/dwd/icon-d2-eps/extracted/extracted/20210222/:/mnt/data/ -it --entrypoint /bin/bash ftt01/pynio \"regridded_icon-d2-eps_germany_icosahedral_single-level_2021022221_027_2d_w_snow.grib2\" 47.6 45.5 11.2 10.2\n",
    "        extractor_cmd = '''docker run --rm -v {c_data_path}:/mnt/data/ ftt01/pynio {lat_max} {lat_min} {lon_max} {lon_min}'''\n",
    "        extractor_cmd = extractor_cmd.format(\n",
    "            c_data_path = input_path,\n",
    "            lat_max=roi_bbox_lat_max,\n",
    "            lat_min=roi_bbox_lat_min,\n",
    "            lon_max=roi_bbox_lon_max,\n",
    "            lon_min=roi_bbox_lon_min\n",
    "        )\n",
    "        process = subprocess.Popen(extractor_cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "                \n",
    "        ## read the output_pathnames as CSV and return the dataframes\n",
    "        t_2m_df = pd.read_csv(input_path+'temperature.csv')\n",
    "        tot_prec_df = pd.read_csv(input_path+'precipitation.csv')\n",
    "        w_snow_df = pd.read_csv(input_path+'snow.csv')\n",
    "    \n",
    "    return t_2m_df, tot_prec_df, w_snow_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_grib2( project_name, c_file, model_name=\"icon-d2-eps\", bbox=[5,17,43,49], output_path=None, logging=None ):\n",
    "\n",
    "    file_path = parent_directory(c_file)\n",
    "    file_name = extract_filename(c_file)\n",
    "\n",
    "    c_cmd = '''docker run --name cropper_{model}_{prj}         --rm -v {file_path}:/home/{model}/data/             --entrypoint cdo                 cdo-ftt_{model} -f grb2                     -sellonlatbox,{lon_min},{lon_max},{lat_min},{lat_max}                         {i_file_name} {o_file_name}'''\n",
    "\n",
    "    complete_cmd = c_cmd.format(\n",
    "        prj=project_name,\n",
    "        file_path=file_path,\n",
    "        model=model_name,\n",
    "        lon_min=bbox[0],\n",
    "        lon_max=bbox[1],\n",
    "        lat_min=bbox[2],\n",
    "        lat_max=bbox[3],\n",
    "        i_file_name=file_name,\n",
    "        o_file_name=\"extracted_\"+file_name )\n",
    "\n",
    "    logging.debug(\"Running CMD: \" + complete_cmd)\n",
    "\n",
    "    subprocess.run( complete_cmd,\n",
    "            shell=True, check=True,\n",
    "            executable='/bin/bash')\n",
    "\n",
    "    # if output_path != None:\n",
    "    #     mv_cmd = \"mv {infile} {outfile}\".format(\n",
    "    #         infile=file_path + \"extracted_\" + file_name,\n",
    "    #         outfile=output_path + file_name\n",
    "    #     )\n",
    "    #     subprocess.run( mv_cmd,\n",
    "    #         shell=True, check=True,\n",
    "    #         executable='/bin/bash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_to_output(input_dir, output_dir, move_keywords=[], mirror=False):\n",
    "\n",
    "    mkNestedDir( output_dir )\n",
    "\n",
    "    logging.info(\"Moving to {output_dir} using keywords: \".format(output_dir=output_dir) + str(move_keywords))\n",
    "\n",
    "    if mirror == True:\n",
    "        files_to_move = glob.glob( input_dir + \"*.grib2\")\n",
    "        for k in move_keywords:\n",
    "            files_to_move = list( set(files_to_move) - set(glob.glob( input_dir + \"{kw}*.grib2\".format(kw=k))) )\n",
    "    else:\n",
    "        files_to_move = []\n",
    "        for k in move_keywords:\n",
    "            files_to_move = files_to_move + list(set(glob.glob( input_dir + \"{kw}*.grib2\".format(kw=k))))\n",
    "\n",
    "    if files_to_move != None:\n",
    "        for f in files_to_move:\n",
    "            logging.debug(\"Moving: \" + f)\n",
    "            try:\n",
    "                shutil.move(f, output_dir + os.path.basename(f))\n",
    "            except IOError as e:\n",
    "                print(\"Unable to copy file. %s\" % e)\n",
    "            except:\n",
    "                print(\"Unexpected error:\", sys.exc_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(\n",
    "    input_path, \n",
    "    days_to_compute, \n",
    "    roi_name, roi_key, basins, \n",
    "    releases, lead_hours, model_ensemble,\n",
    "    output_datetime_format=\"%Y-%m-%dT%H:%M:%SZ%z\", output_timezone='UTC',\n",
    "    input_type=\"ensemble\", output_type=\"mean\", logging=logging):\n",
    "\n",
    "    for day in days_to_compute:\n",
    "\n",
    "        logging.debug( \"Processing date: \" + day )\n",
    "        logging.info( \"Basin: \" + roi_name )\n",
    "\n",
    "        for subbasin in basins:\n",
    "            sb_key = subbasin['key']\n",
    "            logging.info( \"Subbasin: \" + sb_key )\n",
    "            \n",
    "            for release in releases:\n",
    "\n",
    "                c_rel = \"R\" + str(release).zfill(3)\n",
    "                logging.info( \"Release: \" + c_rel )\n",
    "\n",
    "                utc_timezone = pytz.timezone('UTC')\n",
    "                c_start_datetime = utc_timezone.localize( dt.datetime.strptime(\n",
    "                    day + str(int(release)+1).zfill(2), \"%Y%m%d%H\" ) )\n",
    "                c_start_datetime = c_start_datetime.astimezone(output_timezone)\n",
    "                logging.debug( \"Current start datetime: \" + c_start_datetime.strftime(format=\"%Y-%m-%dT%H:%M:%SZ%z\") )\n",
    "\n",
    "                c_path_rel = input_path + roi_key + \"/\" + sb_key + \"/\" + c_rel + \"/\" \n",
    "                path_vars = glob.glob( c_path_rel + \"*/\" )\n",
    "                logging.debug( \"Vars path: \" + str(path_vars) )\n",
    "                \n",
    "                for path_var in path_vars:\n",
    "                    \n",
    "                    c_var = path_var.split('/')[-2]\n",
    "                    logging.info( \"Variable: \" + c_var )\n",
    "                    \n",
    "                    point_matrix = pd.DataFrame()\n",
    "                    for lead_hour in range(lead_hours):\n",
    "\n",
    "                        c_dt = c_start_datetime + dt.timedelta(hours=lead_hour) \n",
    "\n",
    "                        tmp_ens_points = pd.DataFrame( index = [str(n).zfill(3) for n in range( 1, model_ensemble + 1 )] )\n",
    "\n",
    "                        nodes_dir = glob.glob( c_path_rel + c_var + '/' + input_type + \"/*/\" )\n",
    "                        for node_dir in nodes_dir:\n",
    "\n",
    "                            file_to_open = node_dir + day + \".csv\"\n",
    "                            logging.debug( \"Opening: \" + file_to_open )\n",
    "                            # print(file_to_open)\n",
    "\n",
    "                            point_id = str(node_dir.split('/')[-2]).zfill(3)\n",
    "                            logging.debug( \"Current node: \" + point_id )\n",
    "                            # print(point_id)\n",
    "\n",
    "                            try:\n",
    "                                current_file = pd.read_csv( file_to_open, index_col=0 )\n",
    "                                current_file.index = pd.to_datetime( current_file.index, format=output_datetime_format )\n",
    "                                current_file.index = [ output_timezone.localize(\n",
    "                                    idx).astimezone(output_timezone) for idx in current_file.index ]\n",
    "                            except FileNotFoundError as fnf:\n",
    "                                # logging.warning( \"Not found: \" + file_to_open )\n",
    "                                continue\n",
    "\n",
    "                            try:\n",
    "                                current_data = pd.DataFrame( current_file.loc[ c_dt ] )\n",
    "                                tmp_ens_points = pd.concat( [tmp_ens_points,current_data], axis=1 )\n",
    "                            except KeyError as ke:\n",
    "                                # logging.warning( \"Not available: \" + c_dt.strftime(format=\"%Y-%m-%d %H:%M:%S\") )\n",
    "                                continue\n",
    "                            except Exception:\n",
    "                                break\n",
    "                        \n",
    "                        point_matrix[c_dt] = round( tmp_ens_points.mean(axis=1),2 )\n",
    "\n",
    "                        del tmp_ens_points\n",
    "\n",
    "                    mean_matrix = point_matrix.T\n",
    "                    mean_matrix.index = [ dt.datetime.strftime(i, format='%Y-%m-%d %H:%M:%S') for i in mean_matrix.index ]\n",
    "                    mean_matrix.index.name = 'datetime'\n",
    "\n",
    "                    if c_var == 'temperature':\n",
    "                        ### Kelvin to Celsius\n",
    "                        mean_matrix = mean_matrix -  273.15\n",
    "\n",
    "                    if output_type == \"mean\":\n",
    "                        mean_matrix = mean_matrix.mean( axis=1 )\n",
    "                        mean_matrix = round(mean_matrix,2)\n",
    "\n",
    "                        mean_matrix = pd.DataFrame(mean_matrix.values, index=mean_matrix.index, columns=[\"values\"])\n",
    "\n",
    "                    else:\n",
    "                        mean_matrix.columns = [str(m).zfill(3) for m in mean_matrix.columns]\n",
    "\n",
    "                    # output_file = input_path + \"postprocessed/\" + roi_key + '/' + sb_key + '/' + c_rel + '/' + c_var + '/' + \\\n",
    "                    #     output_type + '/spatial_mean/' + day + '.csv'\n",
    "                    output_file = input_path + \"postprocessed/\" + roi_key + '/' + sb_key + '/' + c_rel + '/' + c_var + '/' + \\\n",
    "                        output_type + '/' + day + '.csv'\n",
    "                    mkNestedDir(os.path.dirname(output_file))\n",
    "\n",
    "                    logging.info( \"Saving: \" + output_file )\n",
    "                    mean_matrix.to_csv( output_file )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computation_start = dt.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(configuration_file) as config_file:\n",
    "    configuration = json.load(config_file)\n",
    "\n",
    "    project_name = configuration[\"project_name\"]\n",
    "\n",
    "    provider_name = configuration[\"provider_name\"]\n",
    "    model_name = configuration[\"model\"][\"name\"]\n",
    "    model_ensemble = configuration[\"model\"][\"ensemble\"]\n",
    "    model_resolution = configuration[\"model\"][\"resolution\"]\n",
    "    model_releases = configuration[\"model\"][\"release\"]\n",
    "    model_variables = configuration[\"model\"][\"variables\"]\n",
    "    model_lead_hours = configuration[\"model\"][\"lead_hours\"]\n",
    "    model_timezone_str = configuration[\"model\"][\"timezone\"]\n",
    "    model_timezone = pytz.timezone(model_timezone_str)\n",
    "\n",
    "    if model_ensemble != 1:\n",
    "        input_type = \"ensemble\"\n",
    "    else:\n",
    "        input_type = \"mean\"\n",
    "    \n",
    "    input_path = configuration[\"input_path\"]\n",
    "    output_path = configuration[\"output\"][\"path\"]\n",
    "    mkNestedDir(output_path)\n",
    "    \n",
    "    output_datetime_format = configuration[\"output\"][\"datetime_format\"]\n",
    "    output_timezone_str = configuration[\"output\"][\"timezone\"]\n",
    "    output_timezone = pytz.timezone(output_timezone_str)\n",
    "    output_types = configuration[\"output\"][\"type\"]\n",
    "    output_variables = configuration[\"output\"][\"variables\"]\n",
    "    output_extension = configuration[\"output\"][\"extension\"]\n",
    "    \n",
    "    log_path = Path( configuration[\"log_path\"] )\n",
    "    mkNestedDir(log_path)\n",
    "\n",
    "    regrid = configuration[\"regrid\"]\n",
    "    generate_meta = configuration[\"generate_meta\"]\n",
    "    apply_preprocess = configuration[\"apply_preprocess\"]\n",
    "    apply_postprocess = configuration[\"apply_postprocess\"]\n",
    "\n",
    "    start_date = configuration[\"start_date\"]\n",
    "    try:\n",
    "        start_datetime = dt.datetime.strptime( start_date, \"%Y%m%d\" )\n",
    "    except:\n",
    "        start_datetime = dt.datetime.today()\n",
    "        start_date = dt.datetime.strftime( start_datetime, format=\"%Y%m%d\" )\n",
    "    \n",
    "    end_date = configuration[\"end_date\"]\n",
    "    try:\n",
    "        end_datetime = dt.datetime.strptime( end_date, \"%Y%m%d\" )\n",
    "    except:\n",
    "        end_datetime = dt.datetime.today()\n",
    "        end_date = dt.datetime.strftime( end_datetime, format=\"%Y%m%d\" )\n",
    "    \n",
    "    update_start_date = configuration[\"update_start_date\"]\n",
    "\n",
    "    date_range = []\n",
    "    c_dt = start_datetime\n",
    "    while c_dt <= end_datetime:\n",
    "        date_range.append( dt.datetime.strftime( c_dt, \"%Y%m%d\" ) )\n",
    "        c_dt = c_dt + dt.timedelta(days=1)\n",
    "\n",
    "    roi_config_file = configuration[\"roi_config\"]\n",
    "    with open(roi_config_file) as roi_config_f:\n",
    "        roi_configuration = json.load(roi_config_f)\n",
    "\n",
    "        roi_key = roi_configuration[\"main\"][\"key\"]\n",
    "        roi_name = roi_configuration[\"main\"][\"name\"]\n",
    "        roi_bbox_lat_min = roi_configuration[\"main\"][\"bbox\"][\"lat_min\"]\n",
    "        roi_bbox_lat_max = roi_configuration[\"main\"][\"bbox\"][\"lat_max\"]\n",
    "        roi_bbox_lon_min = roi_configuration[\"main\"][\"bbox\"][\"lon_min\"]\n",
    "        roi_bbox_lon_max = roi_configuration[\"main\"][\"bbox\"][\"lon_max\"]\n",
    "        \n",
    "        basins = roi_configuration[\"basins\"]\n",
    "\n",
    "    roi_config_f.close()\n",
    "\n",
    "    if configuration[\"logging_level\"] == \"info\":\n",
    "        logging_level = logging.INFO\n",
    "    elif configuration[\"logging_level\"] == \"debug\":\n",
    "        logging_level = logging.DEBUG\n",
    "    else:\n",
    "        logging_level = logging.ERROR\n",
    "\n",
    "    email_notification = configuration[\"email\"]\n",
    "    \n",
    "    script_version = configuration[\"script_version\"]\n",
    "\n",
    "config_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_filename = str(log_path) + \"/\" + start_date + \"_\" + end_date + \"_extract_\" + script_version + \".log\"\n",
    "log_filename = str(log_path) + \"/\" + provider_name + \"_extract_\" +  dt.datetime.now().strftime(\"%Y%m%dT%H%M%S\") + \".log\"\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename = log_filename,\n",
    "    format = '%(asctime)s - %(message)s',\n",
    "    filemode = 'a',\n",
    "    level = logging_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info( \"Project name: \" + project_name )\n",
    "logging.info( \"Provider name: \" + provider_name )\n",
    "logging.info( \"Model name: \" + model_name )\n",
    "logging.info( \"Model ensemble number: \" + str(model_ensemble) )\n",
    "logging.info( \"Model releases: \" + str(model_releases) )\n",
    "logging.info( \"Model variables: \" + str(model_variables) )\n",
    "\n",
    "logging.info( \"Output types: \" + str(output_types) )\n",
    "logging.info( \"Output variables: \" + str(output_variables) )\n",
    "\n",
    "logging.info( \"Input path: \" + input_path )\n",
    "logging.info( \"Output path: \" + output_path )\n",
    "logging.info( \"Log filename path: \" + str(log_path) )\n",
    "\n",
    "logging.info( \"Start date: \" + str(start_date) )\n",
    "logging.info( \"End date: \" + str(end_date) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # download grid definition and generate weights\n",
    "# ARG GRID_FILENAME=icon_grid_0047_R19B07_L.nc.bz2\n",
    "# ARG NC_GRID_NUMBER=2\n",
    "# RUN set -ex \\\n",
    "#     && mkdir -p /data/grids/${MODEL_NAME} \\\n",
    "#     && cd /data/grids/${MODEL_NAME} \\\n",
    "#     && wget -O ${MODEL_NAME}_grid.nc.bz2 https://opendata.dwd.de/weather/lib/cdo/${GRID_FILENAME} \\\n",
    "#     && bunzip2 ${MODEL_NAME}_grid.nc.bz2 \\\n",
    "#     && mkdir -p /data/weights/${MODEL_NAME} \\\n",
    "#     && cd /data/weights/${MODEL_NAME} \\\n",
    "#     && echo Generating weights for ${MODEL_NAME} ... \\\n",
    "#     && cdo \\\n",
    "#          gennn,/data/descriptions/${MODEL_NAME}/${MODEL_NAME}_description.txt \\\n",
    "#             -setgrid,/data/grids/${MODEL_NAME}/${MODEL_NAME}_grid.nc:${NC_GRID_NUMBER} \\\n",
    "#             /data/samples/${MODEL_NAME}/${MODEL_NAME}_sample.grib2 \\\n",
    "#             /data/weights/${MODEL_NAME}/${MODEL_NAME}_weights.nc \\\n",
    "#     && cdo \\\n",
    "#          gennn,/data/descriptions/${MODEL_NAME}/${MODEL_NAME}_rotated_description.txt \\\n",
    "#             -setgrid,/data/grids/${MODEL_NAME}/${MODEL_NAME}_grid.nc:${NC_GRID_NUMBER} \\\n",
    "#             /data/samples/${MODEL_NAME}/${MODEL_NAME}_sample.grib2 \\\n",
    "#             /data/weights/${MODEL_NAME}/${MODEL_NAME}_rotated_weights.nc\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the regrid is done using directly with the docker developed by DWD:\n",
    "\n",
    "```\n",
    "docker run --rm \\\n",
    "    --volume ~/mydata:/mydata \\\n",
    "    --env INPUT_FILE=/mydata/my_icon-eps_icosahedral_file.grib2 \\\n",
    "    --env OUTPUT_FILE=/mydata/regridded_regular_lat_lon_output.grib2 \\\n",
    "    deutscherwetterdienst/regrid:icon-eps\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative is the cdo command:\n",
    "\n",
    "```\n",
    "cdo -f grb2 remap,${descriptionFile},${weightsFile} ${input} ${output}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info( \"ROI name: \" + roi_name )\n",
    "\n",
    "# lat_min,lat_max,lon_min,lon_max = bbox_extraction( \n",
    "#     bbox_lat_min=roi_bbox_lat_min,\n",
    "#     bbox_lat_max=roi_bbox_lat_max,\n",
    "#     bbox_lon_min=roi_bbox_lon_min,\n",
    "#     bbox_lon_max=roi_bbox_lon_max,\n",
    "#     spatial_resolution=model_resolution,\n",
    "#     lon_factor=0)\n",
    "\n",
    "# logging.info( \"Main basin BBOX: [{lat_min} {lat_max}, {lon_min} {lon_max}]\".format(\n",
    "#     lat_min=lat_min,\n",
    "#     lat_max=lat_max,\n",
    "#     lon_min=lon_min,\n",
    "#     lon_max=lon_max)\n",
    "#     )\n",
    "\n",
    "logging.info( \"Main basin BBOX: [{lat_min} {lat_max}, {lon_min} {lon_max}]\".format(\n",
    "    lat_min=roi_bbox_lat_min,\n",
    "    lat_max=roi_bbox_lat_max,\n",
    "    lon_min=roi_bbox_lon_min,\n",
    "    lon_max=roi_bbox_lon_max)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = glob.glob( input_path + \"*/\" )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transform precipitation grib2 to 20 bands: reorder\n",
    "1. grib_copy {name}.grib2 {name}_[stepRange].grib2\n",
    "2. mv all that are 0-60 or multiples\n",
    "3. mv all temperature\n",
    "4. mv all snow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dir in dirs:\n",
    "\n",
    "#     curr_date = dir.split('/')[-2]\n",
    "#     if not(curr_date in date_range):\n",
    "#         continue\n",
    "    \n",
    "#     if output_extension == 'csv':\n",
    "\n",
    "#         output_files_path = []\n",
    "#         for vv in model_variables:\n",
    "#             if (vv == 'tot_prec') or (vv == 'TOT_PREC'):\n",
    "#                 c_var = 'precipitation'\n",
    "#             elif vv == 't_2m'  or (vv == 'T_2M'):\n",
    "#                 c_var = 'temperature'\n",
    "#             elif vv == 'w_snow'  or (vv == 'W_SNOW'):\n",
    "#                 c_var = 'snow'\n",
    "#             else:\n",
    "#                 logging.error(\"Not implemented variable: \" + vv)\n",
    "#             for it in output_types:\n",
    "#                 for release in model_releases:\n",
    "#                     c_rel = \"R\" + str(release).zfill(3)\n",
    "\n",
    "#                     for el in basins:\n",
    "#                         logging.debug(\"Check if exist: \" + el['name'])\n",
    "\n",
    "#                         output_files_path.append( \n",
    "#                             output_path + \"postprocessed/{main_basin}/{subbasin}/{release_hour}/{variable}/{input_type}/{c_date}.csv\".format(\n",
    "#                                 main_basin = roi_key,\n",
    "#                                 subbasin = el['key'],\n",
    "#                                 release_hour = c_rel,\n",
    "#                                 variable = c_var,\n",
    "#                                 input_type = it,\n",
    "#                                 c_date = curr_date\n",
    "#                             )\n",
    "#                         )\n",
    "                        \n",
    "#         # print( output_files_path )\n",
    "#         if all([os.path.isfile(f) for f in output_files_path]):\n",
    "#             date_range.remove(curr_date)\n",
    "\n",
    "#     elif output_extension == 'grib2':\n",
    "        \n",
    "#         c_out_file = output_path + \"/\" + curr_date + \"/\"\n",
    "#         try:\n",
    "#             c_files = glob.glob(c_out_file)\n",
    "#             if len(c_files) != 0:\n",
    "#                 date_range.remove(curr_date)\n",
    "#                 continue\n",
    "#         except:\n",
    "#             logging.debug( \"No previous data, extracting: \" + curr_date )\n",
    "\n",
    "#     else:\n",
    "#         logging.error('Not a valid output_extension! [csv,grib2]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec_keys = [\n",
    "    '0-60',\n",
    "    '0-120',\n",
    "    '0-180',\n",
    "    '0-240',\n",
    "    '0-300',\n",
    "    '0-360',\n",
    "    '0-420',\n",
    "    '0-480',\n",
    "    '0-540',\n",
    "    '0-600',\n",
    "    '0-660',\n",
    "    '0-720',\n",
    "    '0-780',\n",
    "    '0-840',\n",
    "    '0-900',\n",
    "    '0-960',\n",
    "    '0-1020',\n",
    "    '0-1080',\n",
    "    '0-1140',\n",
    "    '0-1200',\n",
    "    '0-1260',\n",
    "    '0-1320',\n",
    "    '0-1380',\n",
    "    '0-1440',\n",
    "    '0-1500',\n",
    "    '0-1560',\n",
    "    '0-1620',\n",
    "    '0-1680',\n",
    "    '0-1740',\n",
    "    '0-1800',\n",
    "    '0-1860',\n",
    "    '0-1920',\n",
    "    '0-1980',\n",
    "    '0-2040',\n",
    "    '0-2100',\n",
    "    '0-2160',\n",
    "    '0-2220',\n",
    "    '0-2280',\n",
    "    '0-2340',\n",
    "    '0-2400',\n",
    "    '0-2460',\n",
    "    '0-2520',\n",
    "    '0-2580',\n",
    "    '0-2640',\n",
    "    '0-2700',\n",
    "    '0-2760',\n",
    "    '0-2820',\n",
    "    '0-2880']\n",
    "\n",
    "temp_keys = ['t_2m']\n",
    "snow_keys = ['w_snow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if apply_preprocess == True:\n",
    "\n",
    "    for dir in dirs:\n",
    "\n",
    "        curr_date = dir.split('/')[-2]\n",
    "        if not(curr_date in date_range):\n",
    "            continue\n",
    "        \n",
    "        logging.info(\"Processing: \" + dir)\n",
    "\n",
    "        for model_release in model_releases:\n",
    "\n",
    "            c_keyword_release = [\"_{date}{r}_\".format(date=curr_date,r=str(model_release).zfill(2))]\n",
    "            \n",
    "            if regrid == True:\n",
    "\n",
    "                for c_var in output_variables:\n",
    "\n",
    "                    if c_var == \"precipitation\":\n",
    "                        ### copy the precipitation to to_regrid folder\n",
    "                        regrid_keywords=prec_keys\n",
    "                        reorder_grib2(dir, reorder_keywords=c_keyword_release+[model_variables[\"precipitation\"]], \n",
    "                            regrid_keywords=regrid_keywords)\n",
    "\n",
    "                    elif c_var == \"temperature\":\n",
    "\n",
    "                        ### copy the temperature to_regrid\n",
    "                        move_to_regrid( dir, regrid_keywords=c_keyword_release, vars=[model_variables[\"temperature\"]], mode='any' )\n",
    "                    \n",
    "                    elif c_var == \"snow\":\n",
    "\n",
    "                        ### copy the temperature to_regrid\n",
    "                        move_to_regrid( dir, regrid_keywords=c_keyword_release, vars=[model_variables[\"snow\"]], mode='any' )\n",
    "\n",
    "                if not(model_name == 'icon-eu'):\n",
    "                    remove_unuseful_grib2( dir + 'to_regrid/', keywords=['_000_'] )\n",
    "                    regrid_dir( \n",
    "                        project_name,\n",
    "                        dir + 'to_regrid/', \n",
    "                        model_name=model_name, \n",
    "                        remove_unregridded=True )\n",
    "                    convertion_key = 'regridded'\n",
    "                else:\n",
    "                    convertion_key = ''\n",
    "\n",
    "                files_to_convert = glob.glob( \n",
    "                    dir + 'to_regrid/{convertion_key}*.grib2'.format(\n",
    "                        convertion_key=convertion_key) )\n",
    "            else:\n",
    "                move_to_output(input_dir=dir+'to_regrid/', output_dir=dir,move_keywords=['regridded'])\n",
    "                files_to_convert = glob.glob( dir + '*regular*.grib2' ) +                         glob.glob( dir + '*regridded*.grib2' ) +                                 glob.glob( dir + '*extracted*.grib2' )\n",
    "                # glob.glob( dir + 'to_regrid/*regridded*.grib2' ) + \\\n",
    "                \n",
    "            # release_time = None\n",
    "            remove_unuseful_grib2( dir, keywords=['_000_'], mirror=False )\n",
    "            files_to_convert = [ f for f in files_to_convert if any(word in f for word in prec_keys + temp_keys + snow_keys) ]\n",
    "            files_to_convert = [ f for f in files_to_convert if curr_date+str(model_release).zfill(2) in f ]\n",
    "\n",
    "            if output_extension == 'csv':\n",
    "\n",
    "                # try:\n",
    "                t_2m_df, tot_prec_df, w_snow_df = extract_csv(\n",
    "                    files_to_convert, model_variables,logging, \n",
    "                    roi_bbox=[\n",
    "                        roi_bbox_lat_min,\n",
    "                        roi_bbox_lat_max,\n",
    "                        roi_bbox_lon_min,\n",
    "                        roi_bbox_lon_max], implementation='local')\n",
    "                # except:\n",
    "                #     t_2m_df, tot_prec_df, w_snow_df = extract_csv(\n",
    "                #         files_to_convert, model_variables,logging, \n",
    "                #         roi_bbox=[\n",
    "                #             roi_bbox_lat_min,\n",
    "                #             roi_bbox_lat_max,\n",
    "                #             roi_bbox_lon_min,\n",
    "                #             roi_bbox_lon_max], \n",
    "                #         implementation='docker',\n",
    "                #         input_path=dir)\n",
    "\n",
    "                # if regrid == True:\n",
    "                #     shutil.rmtree( dir + 'to_regrid/' )\n",
    "\n",
    "                ### save all points to a different CSV\n",
    "                if \"precipitation\" in output_variables:\n",
    "                    extract_on_subbasins(\n",
    "                        output_path, tot_prec_df, \"precipitation\", roi_key, model_release, input_type,\n",
    "                        output_datetime_format=output_datetime_format, input_timezone=model_timezone, \n",
    "                        subbasins=basins, model_resolution=model_resolution, generate_meta=generate_meta)\n",
    "                if \"temperature\" in output_variables:\n",
    "                    extract_on_subbasins(\n",
    "                        output_path, t_2m_df, \"temperature\", roi_key, model_release, input_type,\n",
    "                        output_datetime_format=output_datetime_format, input_timezone=model_timezone, \n",
    "                        subbasins=basins, model_resolution=model_resolution, generate_meta=generate_meta)\n",
    "                if \"snow\" in output_variables:\n",
    "                    extract_on_subbasins(\n",
    "                        output_path, w_snow_df, \"snow\", roi_key, model_release, input_type,\n",
    "                        output_datetime_format=output_datetime_format, input_timezone=model_timezone, \n",
    "                        subbasins=basins, model_resolution=model_resolution, generate_meta=generate_meta)\n",
    "\n",
    "            elif output_extension == \"grib2\":\n",
    "                \n",
    "                for file in files_to_convert:\n",
    "\n",
    "                    ## just cut the grib2 and save the cut files for future extraction\n",
    "                    logging.debug(\"Cut to new grib2: \" + file)\n",
    "\n",
    "                    ## run docker container with cdo command to cut the BBOX of the current basin\n",
    "                    ## cdo -f grb2 -sellonlatbox,6.5,14.5,44,47.5 in.grib2 out.grib2\n",
    "                    ## to overwrite the files just use the same name\n",
    "                    c_out_file = output_path + \"/\" + curr_date + \"/\"\n",
    "                    mkNestedDir(c_out_file)\n",
    "                    try:\n",
    "                        crop_grib2(\n",
    "                            project_name,\n",
    "                            file, \n",
    "                            model_name=model_name, \n",
    "                            bbox=[\n",
    "                                roi_bbox_lon_min,\n",
    "                                roi_bbox_lon_max,\n",
    "                                roi_bbox_lat_min,\n",
    "                                roi_bbox_lat_max\n",
    "                                ], \n",
    "                            output_path=c_out_file,\n",
    "                            logging=logging )\n",
    "                    except:\n",
    "                        logging.debug('Unable to convert: ' + file)\n",
    "                        continue    \n",
    "                \n",
    "                move_to_output(input_dir=dir+\"to_regrid/\",output_dir=output_path+curr_date+\"/\",move_keywords=['extracted'])\n",
    "                shutil.rmtree( dir+\"to_regrid/\" )\n",
    "            \n",
    "            else:\n",
    "                logging.error( \"Not a valid extension to extract to..[csv,grib2]\" )\n",
    "        \n",
    "        logging.info(f\"Directory {dir} can be deleted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if update_start_date == True:\n",
    "    new_start_date = (dt.datetime( \n",
    "        end_datetime.year,end_datetime.month,end_datetime.day ) + dt.timedelta(days=1)).strftime(format=\"%Y%m%d\")\n",
    "\n",
    "    logging.info(\"Set up the last date: \" + str(new_start_date))\n",
    "\n",
    "    configuration['start_date'] = new_start_date\n",
    "\n",
    "    with open(configuration_file, 'w') as config_file:\n",
    "        json.dump(configuration, config_file, indent=2)\n",
    "        config_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if apply_postprocess == True:\n",
    "    logging.info('''\\n\n",
    "        ###############################\\n\n",
    "            POSTPROCESS\\n\n",
    "        ###############################\\n''')\n",
    "\n",
    "    for output_type in output_types:\n",
    "\n",
    "        postprocess(\n",
    "            output_path, \n",
    "            date_range, \n",
    "            roi_name, roi_key, basins, \n",
    "            model_releases, model_lead_hours, model_ensemble, \n",
    "            output_datetime_format=output_datetime_format, output_timezone=output_timezone,\n",
    "            input_type=input_type, output_type=output_type, logging=logging\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if email_notification == True:\n",
    "    send_email(\n",
    "        subject=\"DWD extracted!\",\n",
    "        body=\"Started at \" + computation_start.strftime(format=\"%Y-%m-%dT%H:%M:%SZ%z\") + \n",
    "            \"\\nFinish at \" + dt.datetime.now().strftime(format=\"%Y-%m-%dT%H:%M:%SZ%z\") +\n",
    "            \"\\nJSON config: \" + json.dumps(configuration, indent=2, default=str)\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
